# Stage 1: Build the binary
FROM golang:alpine AS builder

# Install required dependencies
RUN apk add --no-cache git build-base cmake bash

# Set the working directory within the container
WORKDIR /app

# Clone the source code from the GitHub repository
RUN git clone https://github.com/ollama/ollama.git

# Edit the gpu/gpu.go file
# Comment out line 182 in gpu/gpu.go
#if cpuVariant == "" && runtime.GOARCH == "amd64" {
#			continue
#		}
RUN cd ollama \ 
	&& sed -i '181,183s/^/\/\//' gpu/gpu.go
	
	
# Define build argument
ARG OLLAMA_CUSTOM_CUDA_DEFS="-DLLAMA_AVX=off -DLLAMA_AVX2=off"

# Use the argument to set environment variables if needed
ENV OLLAMA_CUSTOM_CUDA_DEFS=$OLLAMA_CUSTOM_CUDA_DEFS

# Build the binary with static linking
RUN cd ollama \
	&& go generate ./... \
    && go build -ldflags '-linkmode external -extldflags "-static"' -o .

# Stage 2: Create the final image
FROM alpine

ENV OLLAMA_HOST "0.0.0.0"

# Install required runtime dependencies
RUN apk add --no-cache libstdc++ curl

# Copy the custom entry point script into the container
#COPY Modelfile /Modelfile

# Copy the custom entry point script into the container
COPY entrypoint.sh /entrypoint.sh

# Make the script executable
RUN chmod +x /entrypoint.sh

# Create a non-root user
ARG USER=ollama
ARG GROUP=ollama
RUN addgroup $GROUP && adduser -D -G $GROUP $USER

# Copy the binary from the builder stage
COPY --from=builder /app/ollama/ollama /bin/ollama

ENTRYPOINT ["/entrypoint.sh"]